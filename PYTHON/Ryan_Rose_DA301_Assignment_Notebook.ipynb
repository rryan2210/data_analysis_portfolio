{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59711a08",
   "metadata": {},
   "source": [
    "### LSE Data Analytics Online Career Accelerator \n",
    "\n",
    "# DA301:  Advanced Analytics for Organisational Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90776e",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "You are a data analyst working for Turtle Games, a game manufacturer and retailer. They manufacture and sell their own products, along with sourcing and selling products manufactured by other companies. Their product range includes books, board games, video games and toys. They have a global customer base and have a business objective of improving overall sales performance by utilising customer trends. In particular, Turtle Games wants to understand: \n",
    "- how customers accumulate loyalty points (Week 1)\n",
    "- how useful are remuneration and spending scores data (Week 2)\n",
    "- can social data (e.g. customer reviews) be used in marketing campaigns (Week 3)\n",
    "- what is the impact on sales per product (Week 4)\n",
    "- the reliability of the data (e.g. normal distribution, Skewness, Kurtosis) (Week 5)\n",
    "- if there is any possible relationship(s) in sales between North America, Europe, and global sales (Week 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfeaee",
   "metadata": {},
   "source": [
    "# Week 1 assignment: Linear regression using Python\n",
    "The marketing department of Turtle Games prefers Python for data analysis. As you are fluent in Python, they asked you to assist with data analysis of social media data. The marketing department wants to better understand how users accumulate loyalty points. Therefore, you need to investigate the possible relationships between the loyalty points, age, remuneration, and spending scores. Note that you will use this data set in future modules as well and it is, therefore, strongly encouraged to first clean the data as per provided guidelines and then save a copy of the clean data for future use.\n",
    "\n",
    "## Instructions\n",
    "1. Load and explore the data.\n",
    "    1. Create a new DataFrame (e.g. reviews).\n",
    "    2. Sense-check the DataFrame.\n",
    "    3. Determine if there are any missing values in the DataFrame.\n",
    "    4. Create a summary of the descriptive statistics.\n",
    "2. Remove redundant columns (`language` and `platform`).\n",
    "3. Change column headings to names that are easier to reference (e.g. `renumeration` and `spending_score`).\n",
    "4. Save a copy of the clean DataFrame as a CSV file. Import the file to sense-check.\n",
    "5. Use linear regression and the `statsmodels` functions to evaluate possible linear relationships between loyalty points and age/renumeration/spending scores to determine whether these can be used to predict the loyalty points.\n",
    "    1. Specify the independent and dependent variables.\n",
    "    2. Create the OLS model.\n",
    "    3. Extract the estimated parameters, standard errors, and predicted values.\n",
    "    4. Generate the regression table based on the X coefficient and constant values.\n",
    "    5. Plot the linear regression and add a regression line.\n",
    "6. Include your insights and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea2c71",
   "metadata": {},
   "source": [
    "## 1. Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm \n",
    "from statsmodels.formula.api import ols\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file(s) as reviews.\n",
    "reviews = pd.read_csv('turtle_reviews.csv')\n",
    "\n",
    "# View the DataFrame.\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f96de1-888a-4698-9ec1-23e768642848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any missing values?\n",
    "reviews.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f36bd0-71be-4494-b873-1b1a2f52f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data.\n",
    "print(reviews.shape)\n",
    "print(reviews.columns)\n",
    "print(reviews.dtypes)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e9259-5d92-413a-b9ec-b7ee77e9fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad16658-b4eb-4080-8084-ea043453ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics.\n",
    "reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25344d-3aed-4d27-bb24-2142be9c99ef",
   "metadata": {},
   "source": [
    "## 2. Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b813a-f04f-4c3a-9a11-ad6d7a423525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns.\n",
    "reviews.drop(['language', 'platform'], inplace=True, axis=1)\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71636454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View column names.\n",
    "column_names = list(reviews.columns.values)\n",
    "print(\"Column names:\", column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafd556-c6fa-439b-aac3-0fe332b1eb45",
   "metadata": {},
   "source": [
    "## 3. Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column headers\n",
    "reviews.rename(columns={'remuneration (k£)': 'remuneration', 'spending_score (1-100)': 'spending_score'}, inplace=True)\n",
    "column_names = list(reviews.columns.values)\n",
    "\n",
    "# View column names.\n",
    "print(\"Column names:\", column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c386d53-d38c-4b24-8883-7d2257320036",
   "metadata": {},
   "source": [
    "## 4. Save the DataFrame as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc1746-570a-47cc-a8a9-fe8b6756a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file as output.\n",
    "reviews.to_csv(r'C:\\Users\\roser\\Documents\\LSE Data Analytics\\Course 3\\Assignment\\reviews_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d4f35-c1b3-40ab-ba63-c5fc551f3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import new CSV file with Pandas.\n",
    "reviews_new = pd.read_csv('reviews_new.csv')\n",
    "\n",
    "# View the DataFrame.\n",
    "print(reviews_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd7d5f-2501-4e3e-895c-4a02602e078a",
   "metadata": {},
   "source": [
    "## 5. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7f47e",
   "metadata": {},
   "source": [
    "### 5a) Spending vs Loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75863d52-79df-4200-b044-5542db990fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent variable.\n",
    "y = reviews_new['loyalty_points'] \n",
    "\n",
    "# Define the independent variable.\n",
    "x = reviews_new['spending_score'] \n",
    "\n",
    "# Check for linearity with Matplotlib.\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Insert labels and title.\n",
    "plt.xlabel('Loyalty points', fontsize=10)\n",
    "plt.ylabel('Spending score', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbdd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS model and summary.\n",
    "f = 'y ~ x'\n",
    "test = ols(f, data = reviews_new).fit()\n",
    "\n",
    "# Print the regression table.\n",
    "test.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a55354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the estimated parameters.\n",
    "print(\"Parameters: \", test.params)  \n",
    "\n",
    "# Extract the standard errors.\n",
    "print(\"Standard errors: \", test.bse)  \n",
    "\n",
    "# Extract the predicted values.\n",
    "print(\"Predicted values: \", test.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9798e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X coefficient and the constant to generate the regression table.\n",
    "y_pred = ( -75.0526) +  33.0616 * reviews_new['spending_score'] \n",
    "         \n",
    "# View the output.\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dde67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph with a regression line.\n",
    "\n",
    "# Plot the data points with a scatterplot.\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Plot the regression line (in black).\n",
    "plt.plot(x, y_pred, color='black')\n",
    "\n",
    "# Set the x and y limits on the axes.\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)\n",
    "\n",
    "# Insert labels and title.\n",
    "plt.xlabel('Loyalty points', fontsize=10)\n",
    "plt.ylabel('Spending score', fontsize=10)\n",
    "\n",
    "# View the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0f24f",
   "metadata": {},
   "source": [
    "### 5b) Renumeration vs Loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db590005-b90a-4005-875e-dbec56155229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent variable.\n",
    "y = reviews_new['loyalty_points'] \n",
    "\n",
    "# Define the independent variable.\n",
    "x = reviews_new['remuneration'] \n",
    "\n",
    "# Check for linearity with Matplotlib.\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Insert labels and title.\n",
    "plt.xlabel('Remuneration', fontsize=10)\n",
    "plt.ylabel('Loyalty points', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS model and summary.\n",
    "f = 'y ~ x'\n",
    "test = ols(f, data = reviews_new).fit()\n",
    "\n",
    "# Print the regression table.\n",
    "test.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the estimated parameters.\n",
    "print(\"Parameters: \", test.params)  \n",
    "\n",
    "# Extract the standard errors.\n",
    "print(\"Standard errors: \", test.bse)  \n",
    "\n",
    "# Extract the predicted values.\n",
    "print(\"Predicted values: \", test.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X coefficient and the constant to generate the regression table.\n",
    "y_pred = (-65.6865) + 34.1878 * reviews_new['remuneration'] \n",
    "         \n",
    "# View the output.\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d00e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph with a regression line.\n",
    "\n",
    "# Plot the data points with a scatterplot.\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Plot the regression line (in black).\n",
    "plt.plot(x, y_pred, color='black')\n",
    "\n",
    "# Set the x and y limits on the axes.\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)\n",
    "\n",
    "plt.xlabel('Remuneration', fontsize=10)\n",
    "plt.ylabel('Loyalty points', fontsize=10)\n",
    "\n",
    "# View the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1552d",
   "metadata": {},
   "source": [
    "### 5c) Age vs Loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099274ee-8c86-44dc-a8dc-dfbf91e59728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent variable.\n",
    "y = reviews_new['loyalty_points'] \n",
    "\n",
    "# Define the independent variable.\n",
    "x = reviews_new['age'] \n",
    "\n",
    "plt.xlabel('Age', fontsize=10)\n",
    "plt.ylabel('Loyalty points', fontsize=10)\n",
    "\n",
    "# Check for linearity with Matplotlib.\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9351e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS model and summary.\n",
    "f = 'y ~ x'\n",
    "test = ols(f, data = reviews_new).fit()\n",
    "\n",
    "# Print the regression table.\n",
    "test.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0084f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the estimated parameters.\n",
    "print(\"Parameters: \", test.params)  \n",
    "\n",
    "# Extract the standard errors.\n",
    "print(\"Standard errors: \", test.bse)  \n",
    "\n",
    "# Extract the predicted values.\n",
    "print(\"Predicted values: \", test.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X coefficient and the constant to generate the regression table.\n",
    "y_pred = (40.2034) + -0.0004 * reviews_new['age'] \n",
    "         \n",
    "# View the output.\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph with a regression line.\n",
    "\n",
    "# Plot the data points with a scatterplot.\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Plot the regression line (in black).\n",
    "plt.plot(x, y_pred, color='black')\n",
    "\n",
    "# Set the x and y limits on the axes.\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)\n",
    "\n",
    "\n",
    "plt.xlabel('Age', fontsize=10)\n",
    "plt.ylabel('Loyalty points', fontsize=10)\n",
    "\n",
    "# View the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1fa5b5-ca8f-4ee8-a22e-532142d5cf52",
   "metadata": {},
   "source": [
    "## 6. Observations and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d8150-b5e5-4b96-b138-3437158bc020",
   "metadata": {},
   "source": [
    "***Your observations here...***\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c2fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0e28c75",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c924ec",
   "metadata": {},
   "source": [
    "# Week 2 assignment: Clustering with *k*-means using Python\n",
    "\n",
    "The marketing department also wants to better understand the usefulness of renumeration and spending scores but do not know where to begin. You are tasked to identify groups within the customer base that can be used to target specific market segments. Use *k*-means clustering to identify the optimal number of clusters and then apply and plot the data using the created segments.\n",
    "\n",
    "## Instructions\n",
    "1. Prepare the data for clustering. \n",
    "    1. Import the CSV file you have prepared in Week 1.\n",
    "    2. Create a new DataFrame (e.g. `df2`) containing the `renumeration` and `spending_score` columns.\n",
    "    3. Explore the new DataFrame. \n",
    "2. Plot the renumeration versus spending score.\n",
    "    1. Create a scatterplot.\n",
    "    2. Create a pairplot.\n",
    "3. Use the Silhouette and Elbow methods to determine the optimal number of clusters for *k*-means clustering.\n",
    "    1. Plot both methods and explain how you determine the number of clusters to use.\n",
    "    2. Add titles and legends to the plot.\n",
    "4. Evaluate the usefulness of at least three values for *k* based on insights from the Elbow and Silhoutte methods.\n",
    "    1. Plot the predicted *k*-means.\n",
    "    2. Explain which value might give you the best clustering.\n",
    "5. Fit a final model using your selected value for *k*.\n",
    "    1. Justify your selection and comment on the respective cluster sizes of your final solution.\n",
    "    2. Check the number of observations per predicted class.\n",
    "6. Plot the clusters and interpret the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7299b",
   "metadata": {},
   "source": [
    "## 1. Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1dc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import statsmodels.api as sm \n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file(s) as df2.\n",
    "df2 = pd.read_csv('reviews_new.csv')\n",
    "\n",
    "# View DataFrame.\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c42372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns.\n",
    "df2.drop(['gender', 'age', 'loyalty_points', 'education', 'product', 'review', 'summary'], inplace=True, axis=1)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data.\n",
    "print(df2.shape)\n",
    "print(df2.columns)\n",
    "print(df2.dtypes)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics.\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f971229",
   "metadata": {},
   "source": [
    "## 2. Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot with Seaborn.\n",
    "sns.scatterplot(x='remuneration', y='spending_score', data=df2)\n",
    "\n",
    "plt.title('Scatterplot')\n",
    "plt.xlabel('Remuneration', fontsize=10)\n",
    "plt.ylabel('Spending score', fontsize=10)\n",
    "\n",
    "plt.savefig('Scatterplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot with Seaborn.\n",
    "x = df2[['remuneration', 'spending_score']]\n",
    "\n",
    "sns.pairplot(df2,\n",
    "             vars=x,\n",
    "             diag_kind='kde')\n",
    "\n",
    "plt.savefig('Pairplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d64b28-fc72-4633-af71-2040de573ece",
   "metadata": {},
   "source": [
    "## 3. Elbow and silhoutte methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of clusters: Elbow method.\n",
    "\n",
    "# Import the KMeans class.\n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "# Elbow chart to decide on the number of optimal clusters.\n",
    "ss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i,\n",
    "                    init='k-means++',\n",
    "                    max_iter=300,\n",
    "                    n_init=10,\n",
    "                    random_state=0)\n",
    "    kmeans.fit(x)\n",
    "    ss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow method.\n",
    "plt.plot(range(1, 11),\n",
    "         ss,\n",
    "         marker='o')\n",
    "\n",
    "# Insert labels and title.\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('fdsfsd')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce995702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of clusters: Silhouette method.\n",
    "\n",
    "# Import silhouette_score class from sklearn.\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Find the range of clusters to be used using the Silhouette method.\n",
    "sil = []\n",
    "kmax = 10\n",
    "\n",
    "for k in range(2, kmax+1):\n",
    "    kmeans_s = KMeans(n_clusters=k).fit(x)\n",
    "    labels = kmeans_s.labels_\n",
    "    sil.append(silhouette_score(x,\n",
    "                                labels,\n",
    "                                metric='euclidean'))\n",
    "\n",
    "# Plot the Silhouette method.\n",
    "plt.plot(range(2, kmax+1),\n",
    "         sil,\n",
    "         marker='o')\n",
    "\n",
    "# Insert labels and title.\n",
    "plt.title('The Silhouette Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sil')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0fd764",
   "metadata": {},
   "source": [
    "## 4. Evaluate k-means model at different values of *k*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e340aa-fac0-4cd1-8da0-ee5502a81504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 5 clusters:\n",
    "kmeans = KMeans(n_clusters = 5,\n",
    "                max_iter = 15000,\n",
    "                init='k-means++',\n",
    "                random_state=42).fit(x)\n",
    "\n",
    "clusters = kmeans.labels_\n",
    "x['K-Means Predicted'] = clusters\n",
    "\n",
    "# Plot the predicted.\n",
    "sns.pairplot(x,\n",
    "             hue='K-Means Predicted',\n",
    "             diag_kind= 'kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3307d-3831-4a05-ba52-4cfe24262ec6",
   "metadata": {},
   "source": [
    "## 5. Fit final model and justify your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8802ea-7690-47e6-b23a-0ee483dcde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of observations per predicted class.\n",
    "x['K-Means Predicted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f460666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the K-Means predicte DataFrame.\n",
    "print(x.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d2cf2",
   "metadata": {},
   "source": [
    "## 6. Plot and interpret the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the clusters.\n",
    "# Set plot size.\n",
    "sns.set(rc = {'figure.figsize':(12, 8)})\n",
    "\n",
    "sns.scatterplot(x='spending_score' , \n",
    "                y ='remuneration',\n",
    "                data=x , hue='K-Means Predicted',\n",
    "                palette=['green', 'red', 'blue', 'black', 'orange'])\n",
    "\n",
    "# Insert labels and title.\n",
    "plt.title('Final Model')\n",
    "plt.xlabel('Spending score')\n",
    "plt.ylabel('Remuneration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5625ce",
   "metadata": {},
   "source": [
    "## 7. Discuss: Insights and observations\n",
    "\n",
    "***Your observations here...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f7e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23335aa9",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b58c90",
   "metadata": {},
   "source": [
    "# Week 3 assignment: NLP using Python\n",
    "Customer reviews were downloaded from the website of Turtle Games. This data will be used to steer the marketing department on how to approach future campaigns. Therefore, the marketing department asked you to identify the 15 most common words used in online product reviews. They also want to have a list of the top 20 positive and negative reviews received from the website. Therefore, you need to apply NLP on the data set.\n",
    "\n",
    "## Instructions\n",
    "1. Load and explore the data. \n",
    "    1. Sense-check the DataFrame.\n",
    "    2. You only need to retain the `review` and `summary` columns.\n",
    "    3. Determine if there are any missing values.\n",
    "2. Prepare the data for NLP\n",
    "    1. Change to lower case and join the elements in each of the columns respectively (`review` and `summary`).\n",
    "    2. Replace punctuation in each of the columns respectively (`review` and `summary`).\n",
    "    3. Drop duplicates in both columns (`review` and `summary`).\n",
    "3. Tokenise and create wordclouds for the respective columns (separately).\n",
    "    1. Create a copy of the DataFrame.\n",
    "    2. Apply tokenisation on both columns.\n",
    "    3. Create and plot a wordcloud image.\n",
    "4. Frequency distribution and polarity.\n",
    "    1. Create frequency distribution.\n",
    "    2. Remove alphanumeric characters and stopwords.\n",
    "    3. Create wordcloud without stopwords.\n",
    "    4. Identify 15 most common words and polarity.\n",
    "5. Review polarity and sentiment.\n",
    "    1. Plot histograms of polarity (use 15 bins) for both columns.\n",
    "    2. Review the sentiment scores for the respective columns.\n",
    "6. Identify and print the top 20 positive and negative reviews and summaries respectively.\n",
    "7. Include your insights and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40558b5f",
   "metadata": {},
   "source": [
    "## 1. Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary packages.\n",
    "import nltk \n",
    "import os \n",
    "\n",
    "# nltk.download ('punkt').\n",
    "# nltk.download ('stopwords').\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Import Counter.\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85947561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set as df3.\n",
    "df3 = pd.read_csv('reviews_new.csv')\n",
    "\n",
    "# View DataFrame.\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data set.\n",
    "print(reviews.shape)\n",
    "print(reviews.columns)\n",
    "print(reviews.dtypes)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep necessary columns. Drop unnecessary columns.\n",
    "df3.drop(['gender', 'age', 'remuneration', 'spending_score', 'loyalty_points', 'education', 'product'], inplace=True, axis=1)\n",
    "df3.info()\n",
    "\n",
    "# View DataFrame.\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00736320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if there are any missing values.\n",
    "df3.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bd63b",
   "metadata": {},
   "source": [
    "## 2. Prepare the data for NLP\n",
    "### 2a) Change to lower case and join the elements in each of the columns respectively (review and summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review: Change all to lower case and join with a space.\n",
    "df3['review'] = df3['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Preview the result.\n",
    "df3['review'] .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615be2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Change all to lower case and join with a space.\n",
    "df3['summary'] = df3['summary'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Preview the result.\n",
    "df3['summary'] .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5b39d",
   "metadata": {},
   "source": [
    "### 2b) Replace punctuation in each of the columns respectively (review and summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e14ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the punctuations in review column.\n",
    "# Remove punctuation.\n",
    "df3['review'] = df3['review'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# View output.\n",
    "df3['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the puncuations in summary column.\n",
    "df3['summary'] = df3['summary'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# View output.\n",
    "df3['summary'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6d0b2",
   "metadata": {},
   "source": [
    "### 2c) Drop duplicates in both columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check review column for duplicates\n",
    "df3.review.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates.\n",
    "df4 = df3.drop_duplicates(subset=['review'])\n",
    "\n",
    "# Preview data.\n",
    "df4.reset_index(inplace=True)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696731ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check summary column for duplicates\n",
    "df4.summary.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4953da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates.\n",
    "df5 = df4.drop_duplicates(subset=['summary'])\n",
    "\n",
    "# Preview data.\n",
    "df5.reset_index(inplace=True)\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbbfbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee064a9-dbc4-4b82-b6e7-17c6e441fa05",
   "metadata": {},
   "source": [
    "## 3. Tokenise and create wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame (copy DataFrame).\n",
    "df6 = df5\n",
    "# View DataFrame.\n",
    "\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d4d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String all the comments together in a single variable.\n",
    "# Create an empty string variable.\n",
    "all_comments = ''\n",
    "for i in range(df6.shape[0]):\n",
    "    # Add each comment.\n",
    "    all_comments = all_comments + df6['review'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the colour palette.\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a WordCloud object.\n",
    "word_cloud = WordCloud(width = 1600, height = 900, \n",
    "                background_color ='white',\n",
    "                colormap = 'plasma', \n",
    "                stopwords = 'none',\n",
    "                min_font_size = 10).generate(all_comments) \n",
    "\n",
    "# Plot the WordCloud image.                    \n",
    "plt.figure(figsize = (16, 9), facecolor = None) \n",
    "plt.imshow(word_cloud) \n",
    "plt.axis('off') \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8630ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenisation to both columns.\n",
    "# Tokenise the words.\n",
    "\n",
    "df6['tokens'] = df6['review'].apply(word_tokenize)\n",
    "\n",
    "# Preview data.\n",
    "df6['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenisation to both columns.\n",
    "# Tokenise the words.\n",
    "\n",
    "df6['tokens2'] = df6['summary'].apply(word_tokenize)\n",
    "\n",
    "# Preview data.\n",
    "df6['tokens2'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty list of tokens.\n",
    "all_tokens = []\n",
    "\n",
    "for i in range(df6.shape[0]):\n",
    "    # Add each token to the list.\n",
    "    all_tokens = all_tokens + df6['tokens2'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5abd1",
   "metadata": {},
   "source": [
    "## 4. Frequency distribution and polarity\n",
    "### 4a) Create frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the FreqDist class.\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Calculate the frequency distribution.\n",
    "fdist = FreqDist(all_tokens)\n",
    "\n",
    "# Preview data.\n",
    "fdist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd574d03-a034-454d-b6c5-89aa764c459a",
   "metadata": {},
   "source": [
    "### 4b) Remove alphanumeric characters and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all the alpanum.\n",
    "tokens = [word for word in all_tokens if word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a757d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stopword list.\n",
    "nltk.download ('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create a set of English stopwords.\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Create a filtered list of tokens without stopwords.\n",
    "tokens2 = [x for x in tokens if x.lower() not in english_stopwords]\n",
    "\n",
    "# Define an empty string variable.\n",
    "tokens2_string = ''\n",
    "\n",
    "for value in tokens:\n",
    "    # Add each filtered token word to the string.\n",
    "    tokens2_string = tokens2_string + value + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cfbaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stopword list.\n",
    "nltk.download ('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create a set of English stopwords.\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Create a filtered list of tokens without stopwords.\n",
    "tokens2 = [x for x in tokens if x.lower() not in english_stopwords]\n",
    "\n",
    "# Define an empty string variable.\n",
    "tokens_string = ''\n",
    "\n",
    "for value in tokens:\n",
    "    # Add each filtered token word to the string.\n",
    "    tokens2_string = tokens2_string + value + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68b09f-853e-4c9c-8ff9-ba0b33b8c8e3",
   "metadata": {},
   "source": [
    "### 4c) Create wordcloud without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud without stop words.\n",
    "# Create a WordCloud.\n",
    "wordcloud = WordCloud(width = 1600, height = 900, \n",
    "                background_color ='white', \n",
    "                colormap='plasma', \n",
    "                min_font_size = 10).generate(tokens2_string) \n",
    "\n",
    "# Plot the WordCloud image.                        \n",
    "plt.figure(figsize = (16, 9), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis('off') \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0b15b",
   "metadata": {},
   "source": [
    "### 4d) Identify 15 most common words and polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the 15 most common words.\n",
    "# Import the class.\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Create a frequency distribution object.\n",
    "freq_dist_of_words = FreqDist(tokens2)\n",
    "\n",
    "# Show the fifteen most common elements in the data set.\n",
    "freq_dist_of_words.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59291784-3505-41e5-9914-e4ec8914524b",
   "metadata": {},
   "source": [
    "## 5. Review polarity and sentiment: Plot histograms of polarity (use 15 bins) and sentiment scores for the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84288b8f-aab4-4fff-98d0-aaaf5eef28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the prebuilt rules and values of the vader lexicon.\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the vader class SentimentIntensityAnalyser.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a variable sia to store the SentimentIntensityAnalyser() method.\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fdfa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through a dictionary comprehension to take every cleaned comment\n",
    "df_polarity = {\" \".join(_) : sia.polarity_scores(\" \".join(_)) for _ in df6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d479e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionary results to a DataFrame.\n",
    "polarity = pd.DataFrame(df_polarity).T\n",
    "\n",
    "# View the DataFrame.\n",
    "polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7dac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = df6['review'].apply(lambda x : sia.polarity_scores(x))\n",
    "df6['polarity'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252683e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Create a histogram plot with bins = 15.\n",
    "plt.hist(df6['polarity'], bins=15, alpha)\n",
    "\n",
    "# Histogram of sentiment score\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2523b8",
   "metadata": {},
   "source": [
    "## 6. Identify top 20 positive and negative reviews and summaries respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 negative reviews.\n",
    "\n",
    "\n",
    "# View output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 negative summaries.\n",
    "\n",
    "\n",
    "# View output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 positive reviews.\n",
    "\n",
    "\n",
    "# View output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 positive summaries.\n",
    "\n",
    "\n",
    "# View output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2a108-a8af-4164-9b02-40068c17836d",
   "metadata": {},
   "source": [
    "## 7. Discuss: Insights and observations\n",
    "\n",
    "***Your observations here...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f4c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3ac5e57",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
